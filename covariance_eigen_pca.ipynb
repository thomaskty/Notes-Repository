{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNoh808U16pDZ1IjsOPGZaz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Understanding Cluster Shapes, Covariance Matrices, and Principal Components\n","\n","---\n","## 1. Cluster Shapes: Convex and Isotropic\n","* A cluster is **convex** if any two points within the cluster can be connected by a straight line that remains entirely inside the cluster. Convexity is geometrically simple and aligns well with algorithms like **K-Means**, which rely on Euclidean distance.\n","* Clusters are **isotropic** if they spread out uniformly in all directions. This implies:\n","    * The data has equal variance along all directions.\n","    * The covariance matrix is proportional to the identity matrix.\n","* For isotropic clusters, the covariance matrix: $\\Sigma = \\lambda \\mathbf{I}$ where $\\lambda$ is a scalar and $\\mathbf{I}$ is the identity matrix.\n","\n","* When clusters are non-isotropic:\n","    * Variance is unequal in different directions.\n","    * Covariance exists between features.\n","\n","---\n","\n","## 2. Convexity, Covariance Matrix, and Linear Transformations\n","\n","The **covariance matrix** captures the relationship between features in a dataset. It is symmetric and provides critical information about the spread and orientation of the data.\n","\n","For a dataset $\\mathbf{X}$ with mean $\\bar{\\mathbf{X}}$:\n","$$\n","\\Sigma = \\frac{1}{n-1} (\\mathbf{X} - \\bar{\\mathbf{X}})^T (\\mathbf{X} - \\bar{\\mathbf{X}})\n","$$\n","\n","### As a Linear Transformation\n","The covariance matrix can be treated as a **linear transformation** that scales and rotates data vectors. For a vector $\\mathbf{x}$:\n","$\\mathbf{x'} = \\Sigma \\mathbf{x}$\n","This transformation stretches or compresses $\\mathbf{x}$ along directions determined by the eigenvectors of $\\Sigma$."],"metadata":{"id":"scv-0fXR5uoM"}},{"cell_type":"code","source":["import numpy as np\n","X = np.array([[2, 3], [4, 6], [6, 9]])\n","\n","mean_X = np.mean(X, axis=0)\n","centered_X = X - mean_X\n","cov_matrix = np.cov(centered_X, rowvar=False)\n","print(\"Covariance Matrix:\", cov_matrix)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nltX2pbU5xKL","executionInfo":{"status":"ok","timestamp":1734208872155,"user_tz":-330,"elapsed":383,"user":{"displayName":"Thomaskutty Reji","userId":"11414883861734084745"}},"outputId":"cb8cdaa2-279e-4838-9999-a157c9972804"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Covariance Matrix: [[4. 6.]\n"," [6. 9.]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"CeVn14rM54Vi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## 3. Eigenvalues, Eigenvectors, and Principal Components\n","\n","The **eigenvalues** and **eigenvectors** of the covariance matrix provide:\n","\n","- **Eigenvectors**: Directions of maximum variance (principal directions).\n","- **Eigenvalues**: Magnitudes of variance along those directions.\n","\n","- The eigenvector with the largest eigenvalue points in the direction of the greatest spread.\n","- Smaller eigenvalues correspond to less significant directions.\n","\n","### Proof: Principal Directions Maximize Variance\n","\n","The eigenvectors of $\\Sigma$ maximize the variance of the projected data. For a unit vector $\\mathbf{w}$:\n","$\\text{Variance of projection} = \\mathbf{w}^T \\Sigma \\mathbf{w}$\n","Maximizing this variance under the constraint $\\| \\mathbf{w} \\| = 1$ leads to the eigenvalue equation:\n","$\\Sigma \\mathbf{w} = \\lambda \\mathbf{w}$\n","\n"],"metadata":{"id":"VfrzGuDi59-f"}},{"cell_type":"code","source":["eig_values, eig_vectors = np.linalg.eig(cov_matrix)\n","print(\"Eigenvalues:\", eig_values)\n","print(\"Eigenvectors:\", eig_vectors)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mvbgint354Ns","executionInfo":{"status":"ok","timestamp":1734208893245,"user_tz":-330,"elapsed":382,"user":{"displayName":"Thomaskutty Reji","userId":"11414883861734084745"}},"outputId":"c9f9bb24-66b3-49a2-b58e-5a12037ce903"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Eigenvalues: [ 0. 13.]\n","Eigenvectors: [[-0.83205029 -0.5547002 ]\n"," [ 0.5547002  -0.83205029]]\n"]}]},{"cell_type":"markdown","source":["## 4. Principal Component Analysis (PCA)\n","\n","PCA is a dimensionality reduction technique that uses eigenvalues and eigenvectors to project data onto its most informative directions.\n","\n","1. Compute the covariance matrix $\\Sigma$.\n","2. Perform eigenvalue decomposition.\n","3. Select the top $k$ eigenvectors corresponding to the largest eigenvalues.\n","4. Project the data onto these eigenvectors.\n"],"metadata":{"id":"CVUYrHjo6IYI"}},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","\n","# Fit PCA\n","pca = PCA(n_components=2)\n","pca.fit(X)\n","\n","# Transform the data\n","X_pca = pca.transform(X)\n","print(\"PCA Components:\", pca.components_)\n","print(\"Explained Variance:\", pca.explained_variance_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ekAnTbYr6EX3","executionInfo":{"status":"ok","timestamp":1734208943008,"user_tz":-330,"elapsed":8092,"user":{"displayName":"Thomaskutty Reji","userId":"11414883861734084745"}},"outputId":"1e9638d8-e4b3-4c66-8c88-daa58dde4ae0"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["PCA Components: [[ 0.5547002   0.83205029]\n"," [ 0.83205029 -0.5547002 ]]\n","Explained Variance: [13.  0.]\n"]}]},{"cell_type":"markdown","source":["## 5. Transforming Data Using Covariance Matrix\n","\n","If you multiply data rows by the covariance matrix, the data gets stretched or compressed along the eigenvector directions:\n","$\\mathbf{x'} = \\Sigma \\mathbf{x}$\n","\n","- **Stretching**: The data is scaled along the eigenvectors by the eigenvalues.\n","- **Rotation**: The data aligns with the principal directions.\n"],"metadata":{"id":"gqfvDM_E6Ma-"}},{"cell_type":"code","source":["# Transform data using the covariance matrix\n","transformed_data = np.dot(X, cov_matrix)\n","print(\"Transformed Data:\", transformed_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UxTZQ80W6NH-","executionInfo":{"status":"ok","timestamp":1734208986123,"user_tz":-330,"elapsed":430,"user":{"displayName":"Thomaskutty Reji","userId":"11414883861734084745"}},"outputId":"a7cf3732-3788-4e6e-8d45-9ea510fc866b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Transformed Data: [[ 26.  39.]\n"," [ 52.  78.]\n"," [ 78. 117.]]\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","# Define the covariance matrix (Sigma)\n","Sigma = np.array([[4, 6],\n","                  [6, 9]])\n","\n","# Define the vector x\n","x = np.array([2, 3])\n","\n","# Perform the multiplication of x by Sigma\n","x_prime = np.dot(Sigma, x)\n","\n","# Display the result\n","print(\"Transformed vector x':\", x_prime)\n","\n","# Compute eigenvalues and eigenvectors of Sigma\n","eigenvalues, eigenvectors = np.linalg.eig(Sigma)\n","\n","# Display the eigenvalues and eigenvectors\n","print(\"Eigenvalues:\", eigenvalues)\n","print(\"Eigenvectors:\\n\", eigenvectors)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VzUbBhbb6hTK","executionInfo":{"status":"ok","timestamp":1734209055855,"user_tz":-330,"elapsed":427,"user":{"displayName":"Thomaskutty Reji","userId":"11414883861734084745"}},"outputId":"1e0a0027-047e-44cb-925a-5843322dd52f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Transformed vector x': [26 39]\n","Eigenvalues: [ 0. 13.]\n","Eigenvectors:\n"," [[-0.83205029 -0.5547002 ]\n"," [ 0.5547002  -0.83205029]]\n"]}]},{"cell_type":"markdown","source":["## 7. Summary\n","\n","- **Convexity** ensures that clusters are geometrically simple and compatible with algorithms like K-Means.\n","- **Isotropy** describes uniform spread in all directions, often associated with spherical clusters.\n","- The **covariance matrix** encodes the relationships between features and determines the shape of data clusters.\n","- **Eigenvalues and eigenvectors** of the covariance matrix define the principal directions and variances of the data.\n","- Transforming data with the covariance matrix scales and rotates it along the principal directions.\n"],"metadata":{"id":"2_Fjukju5c29"}},{"cell_type":"code","source":[],"metadata":{"id":"kjHvw7uP5hC3"},"execution_count":null,"outputs":[]}]}