{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54df9b97-ef72-43d6-a85b-32edb985a20a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <font > K means clustering\n",
    "    \n",
    "* The \"cluster center\" is the arithmetic mean of all the points belonging to the cluster.\n",
    "* Each point is closer to its own cluster center than to other cluster centers    \n",
    "    \n",
    "    \n",
    "The time complexity of the K-Means algorithm is typically expressed in terms of the number of iterations and the number of data points. The algorithm consists of the following steps:\n",
    "\n",
    "Initialization: Choose k initial cluster centroids.\n",
    "Assignment: Assign each data point to the nearest centroid.\n",
    "Update: Recalculate the centroids based on the assigned data points.\n",
    "Repeat steps 2-3 until convergence.\n",
    "The time complexity of the K-Means algorithm is often denoted as O(iter * k * n * m), where:\n",
    "\n",
    "iter is the number of iterations until convergence.\n",
    "k is the number of clusters.\n",
    "n is the number of data points.\n",
    "m is the number of dimensions in each data point.\n",
    "In each iteration, the algorithm needs to calculate distances between each data point and each centroid (O(k * n * m)), assign each data point to the nearest centroid (O(n)), and update the centroids (O(k * m)).\n",
    "\n",
    "It's important to note that the actual number of iterations (iter) can vary depending on the convergence criteria. In practice, K-Means often converges quickly, but the worst-case time complexity can be significant for large datasets and high-dimensional spaces.\n",
    "    \n",
    "    \n",
    "#### What is k-means clustering?\n",
    "* K-means clustering is a type of unsupervised machine learning algorithm used to partition a dataset into K clusters, where each data point belongs to the cluster with the nearest mean.\n",
    "#### How does the k-means algorithm work?\n",
    "* The algorithm starts by randomly selecting K centroids, assigns each data point to the cluster with the nearest centroid, recalculates the centroids based on the mean of the points in each cluster, and repeats these steps until convergence.\n",
    "#### What is the role of K in k-means clustering?\n",
    "* K represents the number of clusters that the algorithm should create. It is an input parameter that needs to be specified before running the algorithm.\n",
    "#### How do you choose the optimal value of K?\n",
    "* There are several methods for choosing K, such as the elbow method, silhouette method, or cross-validation. The elbow method involves plotting the cost function against different values of K and choosing the point where the rate of decrease sharply changes.\n",
    "\t\tWhat are the limitations of k-means clustering?\n",
    "\t•\tK-means assumes spherical clusters of similar sizes, is sensitive to initial centroid placement, and may converge to a local minimum. It's also not suitable for non-linear or non-convex clusters.\n",
    "\t\tHow does k-means handle outliers?\n",
    "\t•\tK-means is sensitive to outliers because it relies on the mean to calculate cluster centroids. Outliers can significantly impact the mean, leading to inaccurate cluster assignments. Robust alternatives like K-medoids may be used to address this.\n",
    "\t\tExplain the initialization step in k-means.\n",
    "\t•\tThe initialization step involves randomly selecting K initial centroids. The choice of initial centroids can affect the convergence and final clusters. Some techniques, like k-means++, aim to improve the initialization step.\n",
    "\t\tWhat is the cost function in k-means clustering?\n",
    "\t•\tThe cost function, also known as the distortion or inertia, measures the sum of squared distances between each data point and its assigned cluster centroid. The goal is to minimize this cost function during the iterative steps of the algorithm.\n",
    "\t\tHow does k-means handle categorical data?\n",
    "\t•\tK-means is designed for numerical data, so categorical features may need to be preprocessed. One common approach is to convert categorical variables into numerical representations, but it's essential to consider the nature of the data and the problem context.\n",
    "\t\tCan k-means be used for online/streaming data?\n",
    "\t•\tTraditional k-means is a batch algorithm and is not well-suited for streaming data. However, there are variants like Mini-Batch K-Means that can be used for online or streaming data by updating centroids iteratively as new data points arrive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84d5230-8258-4a89-8de2-1eaddc4eb7ec",
   "metadata": {},
   "source": [
    "* Each instance is assigned to one of the clusters. In the context of clustering, an instance's label is the index of the cluster that this instance gets assigned to by the algorithm; "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e2f537-8ce0-45ca-b17e-ea5856e38ded",
   "metadata": {},
   "source": [
    "Instead of assigning each instance to a single cluster, which is called **hard clustering**, it can be useful to jsut give each instance a score per cluster; this is called **soft clustering**. For example, the score can be the distance between the instance and the centroid, or conversely it can be a **similarity score (affinity)** such as the **Gaussian Radial Basis Funtion** In the Kmeans class , the transform() method measures the distance from each instance to every centroid; "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a808edf-b241-4894-8734-5ba2bdc32b0e",
   "metadata": {},
   "source": [
    "If you have a high-dimensional dataset and you transform it this way, you end up with a k-dimensional dataset; this can be a very efficient non-linear dimensionality reduction technique. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15fe4f6-e641-41d8-a453-cce1a24c70fd",
   "metadata": {},
   "source": [
    "## <font color= 'darkblue '> The K-Means Algorithm \n",
    "We start by placing centroids randomly ( by picking k instances at random and using their locations as centroids). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0043874-f77d-4ed4-94bd-a898bb0ff426",
   "metadata": {},
   "source": [
    "The computational compolexity of the algorithm is generally linear with regards to the number of instances m, the number of clusters k and the number of dimensions n. However, this is only true when the data has a clustering structure. If it does not, then in the worst case scenario the complexity can incrase exponentially with the number of instances. In practice, however this rarely happens, and k menas is gnerally one of the fastest clustering algorithm. \n",
    "\n",
    "Unfortunately although the algorithm is guaranteed to converge, it may not converge to the right solution( it may converge to a local optimum); this depends on the centroid initialization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6b465-9162-4dfa-9b20-c0584e9eb1eb",
   "metadata": {},
   "source": [
    "How we measure the performance of clusters. It is called the models's inertia: this is the mean sqaured distance between each instance and its closest centroid. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a55fafb-8c3e-453a-b8c3-6ff66bf17173",
   "metadata": {},
   "source": [
    "## <font color = \"darkblue\"> What is kmeans ++ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf731530-d5a5-4404-be3a-80d770e4f8fe",
   "metadata": {},
   "source": [
    "Instead of choosing initial cluster centers randomly, choose them smarter!\n",
    "* randomly choose one of the observations to be a cluster center. \n",
    "* for each observation x, determine d(x) where d(x) denotes the minimal distance from x to a current cluster center. \n",
    "* choose next cluster center from the data points, with probability of making an observation x a cluster center proportional to d(x)**2\n",
    "* repeat 2 and 3 until you have chosen the right number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef01708-fbf9-47e1-8062-0695589c8348",
   "metadata": {},
   "source": [
    "KMeans class actually uses this initialization method by default. If you want to force it to use the original  method then you can set the init hyperparameter  to \"random\". You will rarely need to do this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3ddf59-ded9-453a-a55e-437bd37184ae",
   "metadata": {},
   "source": [
    "## <font color = \"darkblue\"> MiniBatch KMeans Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a89dae4-679b-4e06-8b63-60305053bed1",
   "metadata": {},
   "source": [
    "Instead of using the full dataset at each iteration, the algorithm is capable of using mini-batches, moving the centroids just slightly at each iteration. This speeds up the algorithm typically by a factor of 3 or 4 and makes it possible to cluster huge datasets that do not fit in memory. Scikit-Learn implements this algorithm in the MiniBatchKMeans class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25feb41b-a579-4d0e-82b2-0bf03988845f",
   "metadata": {},
   "source": [
    "## <font color = \"darkblue\"> Selecting the optimal number of clusters using elbow rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22bd4f8-0c90-4904-b9c2-dc29a3c1e0b7",
   "metadata": {},
   "source": [
    "As we can see, the inertia drops very quickly as we increase k up to 4, but then it decreases much more slowly as we keep increasing k. This curve has roughly the shape of an arm, and there is an elbow at k=4 so if we did not know better, it would be a good choice; any lower value would be dramatic, while any higher value would not help much, and we might just be splitting perfectly good clusters in half for no reason. \n",
    "\n",
    " * In Elbow method, we compute the WCSS(the summ of sqaured distance between each point and the centroid in a cluster). When we plot the WCSS(within cluster sum of squares) with the k value, the plot looks like an elbow. As the number of clusters increases, the WCSS value will start to decrease.WCSS value is largest when k=1. When we analyze the graph we can see that the graph will rapidly change at a point and thus creating an elbow shape. From this point, the graph starts to move almost parallel to the x-axis. The k value corresponding to this point is the optimal k vlaue or an optimal number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805670ea-2529-4fd8-8769-b31fe0500a2e",
   "metadata": {},
   "source": [
    "## <font color = \"darkblue\"> Silhouette Score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cdad49-d907-4a63-9f5e-e3858c07f814",
   "metadata": {},
   "source": [
    "A more precise approach ( but more computationally expensive ) is to use the silhouette score, which is the mean silhouette coefficient over all the instances. An instance's silhouette coefficient is equal to (b-a)/max(a,b) where a is the mean distances to the other instances in the same cluster ( intra-cluster distance) and b is the mean nearest-cluster distance, that is the mean distance to the instances of the next closest cluster. \n",
    "* The silhouette coefficient can vary between -1 and +1: a coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary, and finally a coefficient close to -1 means that the instance may have been assigned to the wrong cluster. \n",
    " * Intuitively, silhouette score determines whether there are large gaps between each sample and all other samples within the same cluster or across different clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16760e91-d985-441f-b627-9f49ede7ffd5",
   "metadata": {},
   "source": [
    "## <font color = \"darkblue\"> Limits and Merits of kmeans "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8f1703-993c-4005-8bcd-a91014c5951d",
   "metadata": {},
   "source": [
    "It is necessary to run the algorithm several times to avoid sub-optimal solutions, plus we need to specify the number of clusters, which can be quite a hassle. Moreover kmeans does not behave very well when the clusters, have varying sizes, different densities, or non-spherical shapes. So depending on the data, different clustering algorithms may perform better. For example, on these types of elliptical clusters Gaussian mixture models work great. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f04423f-2aa0-4ae0-a5c0-d90587fe4bbc",
   "metadata": {},
   "source": [
    "* it is good to scale the input features before we run kmeans,\n",
    "* sensitive to outliers \n",
    "* sensitive to the initialization process.\n",
    "* we need to provide the number of clusters as an input variable to the algorithm \n",
    "----------------------------------------------------\n",
    "* easy to understand and implement \n",
    "* guranteed convergence "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f26674",
   "metadata": {},
   "source": [
    "## Questions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b0416",
   "metadata": {},
   "source": [
    "##### Is feature Scaling required for the k means algorithm? \n",
    "kmeans typically needs to have some form of normalization done on the datasets to work properly since it is sensitive to both the mean and variance of the datasets.\n",
    "##### Why do we prefer euclidean distance over manhattan distance in kmeans \n",
    "Euclidean distance is preferred over Manhattan distance since Manhattan distance calculates distance only vertically or horizontally due to which it has dimension restrictions.\n",
    "\n",
    "On the contrary, Euclidean distance can be used in any space to calculate the distances between the data points. Since in K means algorithm the data points can be present in any dimension, so Euclidean distance is a more suitable option.\n",
    "\n",
    "#### Optimization Function for the Kmeans\n",
    "The objective of the K-Means algorithm is to find k number of centroids from C1,C2, ... CN, which minimized the within cluster sum of sqaures. \n",
    "#### Ways of avoid the problem of initializatoin sensitivity\n",
    "It is a smart centroid initialization tenchnique. The convergence is faster in KMeans++ \n",
    "#### Convergence \n",
    "when the algorithm reaches the global or local minima, the centroids wont change significantly. The points stay in the same cluster ie. the algorithm has converged at the minima\n",
    "#### Curse of dimensionality in kmeans \n",
    "A key component of K means is that the distance-based computations are directly impacted by a large number of dimensions since the distances between a data point and its nearest and farthest neighbours can become equidistant in high dimension thereby resulting in reduced accuracy of distance-based analysis tools.\n",
    "\n",
    "#### Complexity \n",
    "k*n*d (lesser than 0(n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff521002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
