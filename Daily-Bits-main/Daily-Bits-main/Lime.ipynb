{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e65568f5",
   "metadata": {},
   "source": [
    "# Local Interpretable Model Agnostic Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ff38af",
   "metadata": {},
   "source": [
    "Can we trust prediction? Lime explanation algorithm \n",
    "\n",
    "Can we trust model? Submodular pick algorithm \n",
    "\n",
    "Explains predictions in an interpretable and faithful manner, by learning a surrogate model locally around the prediction. \n",
    "\n",
    "What is an interpretable representation? binary vector indicating presence and absence of words, super-pixels.\n",
    "\n",
    "---\n",
    "\n",
    "## Lime algorithm\n",
    "\n",
    "\n",
    "- start with an instance of interest\n",
    "- get the interpretable representation.\n",
    "- creating perturbed data samples.\n",
    "- get the predictions of the perturbed samples from complex model.\n",
    "- we have the training data and targets.\n",
    "- we give weights to each instance in the new training set.\n",
    "- find the distance between each perturbed instances with the input instance\n",
    "- convert distance to similarity score using squared exponential kernel.\n",
    "- fit the weighted local linear model.\n",
    "- Get the explanations in terms of top k features\n",
    "\n",
    "\n",
    "\n",
    "We use weighted squared loss function for minimization. \n",
    "\n",
    "\n",
    "\n",
    "Sigma is the kernel width. Larger sigma implies that the points which are far away also considered as similar. \n",
    "\n",
    "---\n",
    "\n",
    "## Sub-Modular pick Algorithm\n",
    "\n",
    "\n",
    "- To understand the entire model, one must review the explanations for many of the diverse predictions.\n",
    "- Due to difficulty of selecting the most informative examples, a method called submodular pick algorithm has been suggested.\n",
    "- we run the explanation explanation model on available data set. - get the explanation matrix.\n",
    "- Compute the global importance of individual features be aggregation.\n",
    "- We define a coverage function for an instance, which gives an aggregated measure of all the features in the explanation.\n",
    "- We pick the first instance which has the highest coverage value.\n",
    "- Iteratively add the instances with the highest maximum coverage gain to representive non-redundant explanation set.\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages/Disadvantages\n",
    "\n",
    "- model agnostic explanation\n",
    "- works with multiple data formats.\n",
    "- provides selective explanations.\n",
    "- Helps in model debugging.\n",
    "- Understanding why a customer got rejected from an offer.\n",
    "- Unrealistic data samples.\n",
    "- Kernel width and complexity of the surrogate models should be carefully taken.\n",
    "- if the decision boundary is too non-linear, it can not be sufficiently approximated by a linear model.\n",
    "- interpretable representation will not be powerful enough to explain certain behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04da3995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
