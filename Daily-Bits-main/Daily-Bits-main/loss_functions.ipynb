{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93515825",
   "metadata": {},
   "source": [
    "## **Metrics from L-P Norm**\n",
    "Informally, a norm is a function that accepts as input a vector from our vector space V\n",
    " and spits out a real number that tells us how big that vector is. In order for a function to qualify as a norm, it must first fulfill some properties, so that the results of this metrization process kind of “make sense”. These properties are the following.\n",
    " * Positive Definite \n",
    " * Absolute scalable \n",
    " * Triangular Inequality \n",
    "\n",
    "\\begin{align}\n",
    "\\large MAE = \\frac{1}{m}\\sum^{m}_{i=1}|y_i -\\hat{y}_i| \\large\\\\\n",
    "\\large MSE = \\frac{1}{m}\\sum^{m}_{i=1}(y_i -\\hat{y}_i)^2 \\large\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2c2bc6",
   "metadata": {},
   "source": [
    "##  Entropy\n",
    "In information theory, entropy is a measure of uncertainty or randomness in a set of data. The more unpredictable or random the data, the higher its entropy. \n",
    "\\begin{align}\n",
    "\\large H(X) = -\\sum_{i=1}^{n} P(x_i) \\log_2(P(x_i))\n",
    "\\end{align}\n",
    "* Probability and surprice is inveserly related. \n",
    "* when the probability is 1, the surprice needs to be zero, for this reason we can not say that surprice is 1/p. So surprice is defined as $log(1/p)$.\n",
    "* Entropy can be defined as expected value of surprice. \n",
    "* In summary, the entropy is a measure of the average amount of surprise or information content associated with a set of outcomes. If all outcomes are equally likely, the entropy is maximized, indicating maximum uncertainty. If some outcomes are more likely than others, the entropy is reduced, reflecting a lower level of uncertainty.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b2ff40",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss\n",
    "\n",
    "Cross -entropy loss or log loss measures the performance of a classification model whose output is a probability value between 0 and 1. Cross entropy loss increases as the predicted probability diverges from the actual label.\n",
    "In binary classification, where the number of classes is two, cross entropy can be calculated as follows ;\n",
    "\n",
    "\\begin{align}\n",
    "\\large-(y*log(p) + (1-y) * log(1-p))\\LARGE\n",
    "\\end{align}\n",
    "\n",
    "In multi-classification scenario, we calculate separate loss for each class label per observation and sum the result. \n",
    "\n",
    "\\begin{align}\n",
    "\\large    -\\sum_{c=1}^{M} y*log(p) \\LARGE\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d05bc7",
   "metadata": {},
   "source": [
    "## Joint Entropy \n",
    "\n",
    "Joint entropy is a measure of uncertainty associated with a joint probability distribution of two or more random variables. It is an extension of the concept of entropy, which is used to quantify the uncertainty or information content of a single random variable. Joint entropy takes into account the uncertainty of multiple variables occurring together.\n",
    "\n",
    "For two random variables, X and Y, with a joint probability distribution $P(X,Y)$, \n",
    "the joine entropy, $H(X,Y)$ is given by:\n",
    "\n",
    "\\begin{align}\n",
    "\\large H(X, Y) = -\\sum_{i=1}^{m} \\sum_{j=1}^{n} P(x_i, y_j) \\log_2(P(x_i, y_j))\n",
    "\\end{align}\n",
    "\n",
    "Joint entropy provides a measure of the average amount of uncertainty or information content associated with the joint occurrences of the two random variables. If X and Y are independent, meaning the occurrence of one does not provide information about the other, then the joint entropy is the sum of the individual entropies:\n",
    "$ H(X,Y) = H(X)+H(Y)$ \n",
    "\n",
    "However, if X and Y are dependent, the joint entropy will be less than the sum of the individual entropies, reflecting the reduced uncertainty when the variables are considered together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa692f6",
   "metadata": {},
   "source": [
    "## Mutual Information\n",
    "Mutual information is a measure of the amount of information that knowing the value of one random variable provides about another random variable. It quantifies the reduction in uncertainty about one variable when the value of the other variable is known. Mutual information is widely used in information theory, statistics, and machine learning.\n",
    "\n",
    "for two discreet random variable X,Y, with probability distribution $P(X,Y)$,the mutual information $I(X;Y)$ is given by:\n",
    "\n",
    "\\begin{align}\n",
    "\\large I(X; Y) = \\sum_{i=1}^{m} \\sum_{j=1}^{n} P(x_i, y_j) \\log_2\\left(\\frac{P(x_i, y_j)}{P(x_i)P(y_j)}\\right)\n",
    "\\end{align}\n",
    "\n",
    "Mutual information measures the reduction in uncertainty about one variable (x) given knowledge of the other variable(y). If x and y are independent then $I(x,y) = 0$, indicating that knowing the value of y provides no information about x, and vice versa. On the other hand if X and Y are dependent, $I(x,y)$ is positive, indicating that knowledge of one variable can reduce uncertainty about the other. \n",
    "\n",
    "It is important to note that mutual information is symmetric. It can be used to quantify the relationship between any two random variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7868bb27",
   "metadata": {},
   "source": [
    "## KL Kullback-Leibler Divergence \n",
    "This is also known as relative entropy. Measure of how one probability distribution diverges from a second, expected probability distribution. i its commonly used in information theory to quantify the difference between two probability distributions. \n",
    "\n",
    "\\begin{align}\n",
    "\\large D_{\\text{KL}}(P \\| Q) = \\sum_{i} P(i) \\log\\left(\\frac{P(i)}{Q(i)}\\right)\n",
    "\\end{align}\n",
    "\n",
    "* KL divergence is not symmetric \n",
    "* if p and q are same, then kl divergence is zero\n",
    "* always non-negative \n",
    "* unbounded\n",
    "\n",
    "The interpretation of KL divergence is that it measures the extra average number of bits needed to represent events from P using a code optimized for Q, compared to using a code optimized for P. In other words, it quantifies the inefficiency in using the wrong probability distribution Q to represent the true distribution P."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436cab16",
   "metadata": {},
   "source": [
    "## Huber Loss\n",
    "\n",
    "\\begin{cases} \n",
    "\\large\\frac{1}{2}(y - f(x))^2 & \\large\\text{for } |y - f(x)| \\leq \\delta \\\\\n",
    "\\large\\delta \\cdot (|y - f(x)| - \\frac{1}{2}\\delta) & \\large\\text{otherwise}\n",
    "\\end{cases}\n",
    "\n",
    "The Huber loss is more robust to outliers than MSE because it reduces the impact of large errors on the loss. It avoids the large errors squared in the quadratic term that can dominate the loss in the presence of outliers, making it more resistant to the influence of data points with extremely high residuals.\n",
    "The choice of the $\\delta\\ $ parameter depends on the specific characteristics of the data and the desired balance between the properties of MSE and MAE. \n",
    "A smaller $\\delta\\ $ makes the loss more quadratic, while a larger $\\delta\\$ makes it more linear.\n",
    "\n",
    "In summary, Huber loss is a compromise loss function that combines the advantages of both mean squared error and mean absolute error, providing a good balance between sensitivity to outliers and differentiability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7742bafd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
