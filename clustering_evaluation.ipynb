{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Silhouette Score for Clustering Evaluation\n",
        "\n",
        "The **Silhouette score** is a widely used metric for evaluating the quality of clusters in a clustering algorithm. It measures how similar a point is to its own cluster compared to other clusters. The higher the Silhouette score, the better defined the clusters are.\n",
        "\n",
        "## Key Concept\n",
        "\n",
        "The Silhouette score combines both cohesion (how close points in a cluster are to each other) and separation (how well-separated the clusters are). For each data point, the score compares the average distance to other points within the same cluster (cohesion) to the average distance to points in the nearest cluster (separation). The Silhouette score is calculated for all points in the dataset, and the overall score is the average of individual point scores.\n",
        "\n",
        "### Silhouette Score Equation\n",
        "\n",
        "The **Silhouette score** for an individual point \\( i \\) is defined as:\n",
        "\n",
        "\\begin{equation}\n",
        "s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}\n",
        "\\end{equation}\n",
        "\n",
        "Where:\n",
        "- $ a(i)$ is the **average distance** between point \\( i \\) and all other points in the same cluster. This measures cohesion.\n",
        "- $ b(i) $ is the **minimum average distance** between point \\( i \\) and all points in any other cluster. This measures separation.\n",
        "- $ \\max(a(i), b(i)) $ is used to normalize the score, ensuring that it falls between -1 and 1.\n",
        "\n",
        "### Intuition Behind the Formula\n",
        "\n",
        "- **Cohesion** $a(i)$ measures how well the point \\( i \\) fits within its cluster. A lower value of $a(i)$ means that the point is close to other points in the cluster.\n",
        "- **Separation** $b(i)$ measures how well-separated point $i$ is from the other clusters. A higher value of $b(i)$ means that the point is far from the nearest neighboring cluster.\n",
        "\n",
        "- The **Silhouette score** $s(i)$ can take values between -1 and 1:\n",
        "  - A score close to **1** indicates that the point is well clustered (both close to its own cluster and far from others).\n",
        "  - A score close to **0** indicates that the point lies on or near the boundary between two clusters.\n",
        "  - A score close to **-1** indicates that the point may have been assigned to the wrong cluster.\n",
        "\n",
        "### Silhouette Score for the Entire Dataset\n",
        "\n",
        "The **Silhouette score for the entire dataset** is the average of the Silhouette scores of all individual points:\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{Silhouette Score} = \\frac{1}{n} \\sum_{i=1}^{n} s(i)\n",
        "\\end{equation}\n",
        "\n",
        "Where:\n",
        "- \\( n \\) is the total number of data points.\n",
        "- \\( s(i) \\) is the individual Silhouette score for point \\( i \\).\n",
        "\n",
        "This overall score provides an indication of how well the clustering algorithm performed.\n",
        "\n",
        "### Interpretation of the Silhouette Score\n",
        "\n",
        "- **A high Silhouette score** (close to 1) means the clustering configuration is very good, with distinct, well-separated clusters.\n",
        "- **A low Silhouette score** (close to 0) indicates that some data points may be on the boundary of two clusters, suggesting a poor clustering result.\n",
        "- **A negative Silhouette score** (close to -1) implies that many points are likely assigned to the wrong clusters, and the clustering is highly ineffective.\n",
        "\n",
        "## Strengths of the Silhouette Score\n",
        "\n",
        "- **Interpretability**: The Silhouette score provides a simple and intuitive measure of clustering quality.\n",
        "- **Cluster Validation**: It can be used to assess the validity of different clustering solutions (e.g., comparing the Silhouette scores of different values of \\( k \\) in K-Means clustering).\n",
        "- **Range of Values**: The score is bounded between -1 and 1, making it easy to interpret.\n",
        "\n",
        "## Limitations of the Silhouette Score\n",
        "\n",
        "- **Assumes Convex Clusters**: The Silhouette score assumes that clusters are convex and isotropic, which may not always be the case, especially for non-globular clusters.\n",
        "- **Sensitive to Outliers**: The score can be sensitive to outliers or noise, as these points can distort the cohesion and separation calculations.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "The **Silhouette score** is an effective and widely used metric for evaluating the quality of clustering. It provides a balance between cohesion and separation, and it can help determine the optimal number of clusters in a dataset. While it is useful for many clustering tasks, it may not perform well with irregularly shaped clusters or datasets with significant noise.\n"
      ],
      "metadata": {
        "id": "Wxuesmlq2mIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation).\n",
        "# silhouette_score = (b-a)/max(b,a)\n",
        "# a - cohesion\n",
        "# b - separation"
      ],
      "metadata": {
        "id": "MYSRuAJxoSr1"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "def calculate_silhouette_score(data, labels):\n",
        "    \"\"\"\n",
        "    Custom function to explain the silhouette score.\n",
        "\n",
        "    Parameters:\n",
        "        data (numpy.ndarray): The dataset (2D array).\n",
        "        labels (list or numpy.ndarray): Cluster labels for each data point.\n",
        "\n",
        "    Returns:\n",
        "        float: The silhouette score.\n",
        "    \"\"\"\n",
        "    n_samples = len(data)\n",
        "    silhouette_values = []\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        current_cluster = labels[i]\n",
        "\n",
        "        # Points in the same cluster\n",
        "        same_cluster = data[labels == current_cluster]\n",
        "\n",
        "        # Points in other clusters\n",
        "        other_clusters = data[labels != current_cluster]\n",
        "\n",
        "        # Compute average intra-cluster distance (a) : cohesion\n",
        "        a = np.mean(np.linalg.norm(same_cluster - data[i], axis=1))\n",
        "\n",
        "        # Compute average nearest-cluster distance (b) : separation\n",
        "        b = np.mean(np.linalg.norm(other_clusters - data[i], axis=1))\n",
        "\n",
        "        # Silhouette value for the point\n",
        "        silhouette_values.append((b - a) / max(a, b))\n",
        "\n",
        "    # Average silhouette score\n",
        "    return np.mean(silhouette_values)\n",
        "\n",
        "# Example dataset (2D points for visualization)\n",
        "data = np.array([\n",
        "    [1, 2], [2, 3], [3, 4],   # Cluster 1\n",
        "    [8, 8], [9, 9], [8, 9],   # Cluster 2\n",
        "    [15, 14], [16, 15], [15, 16] # Cluster 3\n",
        "])"
      ],
      "metadata": {
        "id": "cC4nmMBci-mR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0Ylqeo9kNpV",
        "outputId": "b69b76bb-e87d-454e-e77b-32846ec3b768"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  2],\n",
              "       [ 2,  3],\n",
              "       [ 3,  4],\n",
              "       [ 8,  8],\n",
              "       [ 9,  9],\n",
              "       [ 8,  9],\n",
              "       [15, 14],\n",
              "       [16, 15],\n",
              "       [15, 16]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply KMeans clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42).fit(data)\n",
        "labels = kmeans.labels_\n"
      ],
      "metadata": {
        "id": "vUGBOANKkSsP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute silhouette score using sklearn\n",
        "sil_score = silhouette_score(data, labels)\n",
        "print(f\"Silhouette Score using sklearn: {sil_score:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH1Mdc59kWTf",
        "outputId": "aaa1b051-470f-487c-c1b8-b1dea0faf750"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Score using sklearn: 0.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4cYLB8pkY56",
        "outputId": "d722e960-eac6-4889-e38d-0d8471554b4d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 2, 2, 0, 0, 0, 1, 1, 1], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Points in the same cluster\n",
        "current_cluster = 2\n",
        "same_cluster = data[labels == current_cluster]\n",
        "# Points in other clusters\n",
        "other_clusters = data[labels != current_cluster]"
      ],
      "metadata": {
        "id": "PEweMy7rkbZy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "same_cluster,other_clusters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMPr4k10krj5",
        "outputId": "c636e728-adc9-4692-a8a8-87cedaa7e7b2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[1, 2],\n",
              "        [2, 3],\n",
              "        [3, 4]]),\n",
              " array([[ 8,  8],\n",
              "        [ 9,  9],\n",
              "        [ 8,  9],\n",
              "        [15, 14],\n",
              "        [16, 15],\n",
              "        [15, 16]]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "same_cluster-data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wH2WQnP8kwoG",
        "outputId": "3ae64dfd-ab4e-4c4e-c6c2-1ac061a0e35c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0],\n",
              "       [1, 1],\n",
              "       [2, 2]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.linalg.norm(same_cluster-data[0],axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To0YrWxslJ8E",
        "outputId": "2e45e122-c09f-45d9-b544-54ec33ef78c6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 1.41421356, 2.82842712])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sqrt(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GF0zY3ClWY7",
        "outputId": "7309fc33-c356-45b6-8886-d3b2ec0f6bb0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.4142135623730951"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVSpiQzymwHz",
        "outputId": "1b102f80-df19-4f55-ca34-8ea5e4091f6d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  2],\n",
              "       [ 2,  3],\n",
              "       [ 3,  4],\n",
              "       [ 8,  8],\n",
              "       [ 9,  9],\n",
              "       [ 8,  9],\n",
              "       [15, 14],\n",
              "       [16, 15],\n",
              "       [15, 16]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# axis=0 , feature wise operation\n",
        "# axis=1 , row wise operation\n",
        "data.sum(axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeSzGVaMm_l7",
        "outputId": "958727a0-2d5a-4a6f-d1ac-a4fa9e3242e2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3,  5,  7, 16, 18, 17, 29, 31, 31])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute silhouette score using custom function\n",
        "custom_score = calculate_silhouette_score(data, labels)\n",
        "print(f\"Silhouette Score using custom function: {custom_score:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqUp1PDEnxcx",
        "outputId": "99522ad9-eb60-4445-a842-7d8316acf729"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Score using custom function: 0.91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Explanation with numeric example\n",
        "example_point = data[0]  # First data point\n",
        "example_cluster = labels[0]  # Cluster of the first data point\n",
        "same_cluster_points = data[labels == example_cluster]\n",
        "other_cluster_points = data[labels != example_cluster]\n",
        "\n",
        "# Average intra-cluster distance (a)\n",
        "a_example = np.mean(np.linalg.norm(same_cluster_points - example_point, axis=1))\n",
        "# Average nearest-cluster distance (b)\n",
        "b_example = np.mean(np.linalg.norm(other_cluster_points - example_point, axis=1))\n",
        "\n",
        "print(\"\\nExplanation with Numeric Example:\")\n",
        "print(f\"Point: {example_point}\")\n",
        "print(f\"same cluster ponints :\\n{same_cluster_points}\")\n",
        "print(f\"other cluster ponints :\\n{other_cluster_points}\")\n",
        "print(f'same cluster difference :\\n{same_cluster_points-example_point}')\n",
        "print(f'other cluster difference :\\n{other_cluster_points-example_point}')\n",
        "\n",
        "print('*'*50)\n",
        "print(f'norm of the vectors  a : \\n{np.linalg.norm(same_cluster_points-example_point,axis=1)}')\n",
        "print(f'norm of the vectors  b : \\n{np.linalg.norm(other_cluster_points-example_point,axis=1)}')\n",
        "print(f'cohesion = {np.mean(np.linalg.norm(same_cluster_points-example_point,axis=1))}')\n",
        "print(f'separation = {np.mean(np.linalg.norm(other_cluster_points-example_point,axis=1))}')\n",
        "\n",
        "print('*'*50)\n",
        "print(f\"Average intra-cluster distance (a): {a_example:.2f}\")\n",
        "print(f\"Average nearest-cluster distance (b): {b_example:.2f}\")\n",
        "print(f\"Silhouette value for the point: {(b_example - a_example) / max(a_example, b_example):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-R6dPo1kF-c",
        "outputId": "fd9cf97e-f816-483a-ab63-ea150aa8fb38"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation with Numeric Example:\n",
            "Point: [1 2]\n",
            "same cluster ponints :\n",
            "[[1 2]\n",
            " [2 3]\n",
            " [3 4]]\n",
            "other cluster ponints :\n",
            "[[ 8  8]\n",
            " [ 9  9]\n",
            " [ 8  9]\n",
            " [15 14]\n",
            " [16 15]\n",
            " [15 16]]\n",
            "same cluster difference :\n",
            "[[0 0]\n",
            " [1 1]\n",
            " [2 2]]\n",
            "other cluster difference :\n",
            "[[ 7  6]\n",
            " [ 8  7]\n",
            " [ 7  7]\n",
            " [14 12]\n",
            " [15 13]\n",
            " [14 14]]\n",
            "**************************************************\n",
            "norm of the vectors  a : \n",
            "[0.         1.41421356 2.82842712]\n",
            "norm of the vectors  b : \n",
            "[ 9.21954446 10.63014581  9.89949494 18.43908891 19.84943324 19.79898987]\n",
            "cohesion = 1.4142135623730951\n",
            "separation = 14.639449539287922\n",
            "**************************************************\n",
            "Average intra-cluster distance (a): 1.41\n",
            "Average nearest-cluster distance (b): 14.64\n",
            "Silhouette value for the point: 0.90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Davies-Bouldin Score"
      ],
      "metadata": {
        "id": "0pWs9dWS3RUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Davies-Bouldin Score** evaluates clustering quality by measuring the average similarity between clusters. Lower scores indicate better clustering.\n",
        "\n",
        "The formula is:\n",
        "\n",
        "\\begin{equation}\n",
        "DB = \\frac{1}{k} \\sum_{i=1}^{k} \\max_{j \\neq i} \\left( \\frac{\\sigma_i + \\sigma_j}{d_{ij}} \\right)\n",
        "\\end{equation}\n",
        "\n",
        "Where:\n",
        "- $k$: The number of clusters.\n",
        "- $\\sigma_i$: Scatter within cluster $i$, defined as the average distance of all points in cluster $i$ to the cluster centroid $c_i$:\n",
        "  \\begin{equation}\n",
        "  \\sigma_i = \\frac{1}{|C_i|} \\sum_{x \\in C_i} \\text{dist}(x, c_i)\n",
        "  \\end{equation}\n",
        "  Here, $|C_i|$ is the number of points in cluster $i$.\n",
        "\n",
        "- $d_{ij}$: Distance between centroids of clusters $i$ and $j$, typically computed as:\n",
        "  \\begin{equation}\n",
        "  d_{ij} = \\text{dist}(c_i, c_j)\n",
        "  \\end{equation}\n",
        "\n",
        "**Steps to Compute Davies-Bouldin Score**:\n",
        "1. For each cluster $i$, calculate its scatter $\\sigma_i$.\n",
        "2. For each pair of clusters $(i, j)$, compute the **similarity**:\n",
        "   \\begin{equation}\n",
        "   R_{ij} = \\frac{\\sigma_i + \\sigma_j}{d_{ij}}\n",
        "   \\end{equation}\n",
        "3. For each cluster $i$, find the maximum $R_{ij}$ across all other clusters $j \\neq i$.\n",
        "4. Average these maximum values over all clusters to get $DB$:\n",
        "   \\begin{equation}\n",
        "   DB = \\frac{1}{k} \\sum_{i=1}^k \\max_{j \\neq i} R_{ij}\n",
        "   \\end{equation}\n",
        "\n",
        "**Interpretation**:\n",
        "- **Lower $DB$**: Clusters are compact (low $\\sigma$) and well-separated (high $d_{ij}$).\n",
        "- **Higher $DB$**: Indicates poor clustering (overlapping or dispersed clusters).\n"
      ],
      "metadata": {
        "id": "JknpZCgs3RH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import davies_bouldin_score\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "def calculate_davies_bouldin_score(data, labels):\n",
        "    \"\"\"\n",
        "    Custom function to explain the Davies-Bouldin score.\n",
        "\n",
        "    Parameters:\n",
        "        data (numpy.ndarray): The dataset (2D array).\n",
        "        labels (list or numpy.ndarray): Cluster labels for each data point.\n",
        "\n",
        "    Returns:\n",
        "        float: The Davies-Bouldin score.\n",
        "    \"\"\"\n",
        "    n_clusters = len(np.unique(labels))\n",
        "    cluster_means = []  # Centroids of clusters\n",
        "    intra_cluster_distances = []  # Scatter within clusters\n",
        "\n",
        "    # Compute cluster centroids and intra-cluster distances\n",
        "    for cluster in range(n_clusters):\n",
        "        cluster_points = data[labels == cluster]\n",
        "        centroid = np.mean(cluster_points, axis=0)\n",
        "        cluster_means.append(centroid)\n",
        "        scatter = np.mean(np.linalg.norm(cluster_points - centroid, axis=1))\n",
        "        intra_cluster_distances.append(scatter)\n",
        "\n",
        "    # Compute pairwise Davies-Bouldin components\n",
        "    db_values = []\n",
        "    for i in range(n_clusters):\n",
        "        max_ratio = 0\n",
        "        for j in range(n_clusters):\n",
        "            if i != j:\n",
        "                inter_cluster_distance = np.linalg.norm(cluster_means[i] - cluster_means[j])\n",
        "                ratio = (intra_cluster_distances[i] + intra_cluster_distances[j]) / inter_cluster_distance\n",
        "                max_ratio = max(max_ratio, ratio)\n",
        "        db_values.append(max_ratio)\n",
        "\n",
        "    # Davies-Bouldin score is the average of the maximum ratios\n",
        "    return np.mean(db_values)\n",
        "\n",
        "# Example dataset (2D points for visualization)\n",
        "data = np.array([\n",
        "    [1, 2], [2, 3], [3, 4],   # Cluster 1\n",
        "    [8, 8], [9, 9], [8, 9],   # Cluster 2\n",
        "    [15, 14], [16, 15], [15, 16] # Cluster 3\n",
        "])\n",
        "\n",
        "# Apply KMeans clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42).fit(data)\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Compute Davies-Bouldin score using sklearn\n",
        "db_score = davies_bouldin_score(data, labels)\n",
        "print(f\"Davies-Bouldin Score using sklearn: {db_score:.2f}\")\n",
        "\n",
        "# Compute Davies-Bouldin score using custom function\n",
        "custom_db_score = calculate_davies_bouldin_score(data, labels)\n",
        "print(f\"Davies-Bouldin Score using custom function: {custom_db_score:.2f}\")\n",
        "\n",
        "# Explanation with numeric example\n",
        "print(\"\\nExplanation with Numeric Example:\")\n",
        "cluster_1_points = data[labels == 0]\n",
        "cluster_2_points = data[labels == 1]\n",
        "centroid_1 = np.mean(cluster_1_points, axis=0)\n",
        "centroid_2 = np.mean(cluster_2_points, axis=0)\n",
        "\n",
        "# Intra-cluster distances (scatter)\n",
        "scatter_1 = np.mean(np.linalg.norm(cluster_1_points - centroid_1, axis=1))\n",
        "scatter_2 = np.mean(np.linalg.norm(cluster_2_points - centroid_2, axis=1))\n",
        "\n",
        "# Inter-cluster distance\n",
        "inter_cluster_distance = np.linalg.norm(centroid_1 - centroid_2)\n",
        "\n",
        "# Davies-Bouldin ratio for cluster 1 and 2\n",
        "db_ratio = (scatter_1 + scatter_2) / inter_cluster_distance\n",
        "\n",
        "print(f\"Cluster 1 Centroid: {centroid_1}\")\n",
        "print(f\"Cluster 2 Centroid: {centroid_2}\")\n",
        "print(f\"Cluster 1 Scatter: {scatter_1:.2f}\")\n",
        "print(f\"Cluster 2 Scatter: {scatter_2:.2f}\")\n",
        "print(f\"Inter-Cluster Distance: {inter_cluster_distance:.2f}\")\n",
        "print(f\"Davies-Bouldin Ratio (Cluster 1 & 2): {db_ratio:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxvy2KyIqcbU",
        "outputId": "a71b1225-b372-4412-9d38-59267f7a171b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Davies-Bouldin Score using sklearn: 0.18\n",
            "Davies-Bouldin Score using custom function: 0.18\n",
            "\n",
            "Explanation with Numeric Example:\n",
            "Cluster 1 Centroid: [8.33333333 8.66666667]\n",
            "Cluster 2 Centroid: [15.33333333 15.        ]\n",
            "Cluster 1 Scatter: 0.65\n",
            "Cluster 2 Scatter: 0.92\n",
            "Inter-Cluster Distance: 9.44\n",
            "Davies-Bouldin Ratio (Cluster 1 & 2): 0.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps for Multiple Clusters\n",
        "\n",
        "* Compute Centroids for Each Cluster: Find the centroid for each cluster by taking the mean of all points in the cluster.\n",
        "\n",
        "* Calculate Scatter (σi) for Each Cluster: For each cluster, compute the average distance of all points in that cluster to the cluster’s centroid.\n",
        "\n",
        "* Compute Pairwise Centroid Distances (dij): For each pair of clusters ii and jj, calculate the distance between their centroids.\n",
        "\n",
        "* Compute Rij for All Cluster Pairs: For each pair of clusters ii and jj, compute:\n",
        "    Rij=σi+σjdij\n",
        "    Rij​=dij​σi​+σj​​\n",
        "\n",
        "* This measures the similarity between clusters ii and jj.\n",
        "\n",
        "* Find the Maximum Rij for Each Cluster: For each cluster ii, find the maximum RijRij​ across all other clusters jj. This represents how \"bad\" the separation is for cluster ii.\n",
        "\n",
        "* Compute the Average of Maximum Rij: Take the average of the maximum RijRij​ values across all clusters to get the Davies-Bouldin Score:"
      ],
      "metadata": {
        "id": "Eo5pBpTS5V0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Silhoette Score: The Silhouette Score measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation).**\n",
        "\n",
        "**Davies-Bouldin (DB) Score : evaluates the average similarity between each cluster and its most similar cluster.**"
      ],
      "metadata": {
        "id": "s_55VNj-50UT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calinski-Harabasz (CH) Score for Clustering Evaluation\n",
        "\n",
        "The **Calinski-Harabasz index** (CH score) is a metric used to evaluate the quality of clustering results. It measures how well-separated and compact the clusters are. A higher CH score generally indicates better clustering.\n",
        "\n",
        "## Key Concept\n",
        "\n",
        "The CH score is based on the dispersion of data points within clusters and between clusters. The goal is to have well-separated clusters (large between-cluster dispersion) and compact clusters (small within-cluster dispersion). The score is calculated as the ratio of between-cluster dispersion to within-cluster dispersion.\n",
        "\n",
        "### Between-Cluster Dispersion\n",
        "\n",
        "The **between-cluster dispersion matrix** \\( B_k \\) quantifies the spread of the centroids of different clusters. It is calculated by considering the distances between each cluster's centroid and the global centroid (mean of all data points). The more separated the cluster centroids are from the global centroid, the higher the between-cluster dispersion.\n",
        "\n",
        "### Within-Cluster Dispersion\n",
        "\n",
        "The **within-cluster dispersion matrix** \\( W_k \\) measures the compactness of each cluster. It is the sum of squared distances between each data point in a cluster and the cluster's centroid. Smaller within-cluster dispersion indicates that the data points are tightly grouped around their respective centroids.\n",
        "\n",
        "### Calinski-Harabasz Index Equation\n",
        "\n",
        "The Calinski-Harabasz index (CH score) is given by the following formula:\n",
        "\n",
        "\\[\n",
        "CH = \\frac{\\text{tr}(B_k) / (k - 1)}{\\text{tr}(W_k) / (n - k)}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( B_k \\) is the **between-cluster dispersion matrix**.\n",
        "- \\( W_k \\) is the **within-cluster dispersion matrix**.\n",
        "- \\( n \\) is the total number of data points.\n",
        "- \\( k \\) is the number of clusters.\n",
        "- \\( \\text{tr}() \\) represents the trace of the matrix (sum of diagonal elements).\n",
        "\n",
        "### Intuition Behind the Formula\n",
        "\n",
        "- The numerator represents the between-cluster dispersion normalized by the number of clusters \\( k \\).\n",
        "- The denominator represents the within-cluster dispersion normalized by the number of data points \\( n \\).\n",
        "- A higher CH score means that the clusters are well-separated (larger between-cluster dispersion) and compact (smaller within-cluster dispersion).\n",
        "\n",
        "## Calculation Steps\n",
        "\n",
        "1. **Compute the Centroids**:\n",
        "   - Calculate the centroid for each cluster. The centroid is the mean of all data points in the cluster.\n",
        "   - Compute the global centroid (mean of all data points in the dataset).\n",
        "\n",
        "2. **Calculate the Within-Cluster Dispersion**:\n",
        "   - For each cluster, calculate the sum of squared distances between each point and the cluster's centroid.\n",
        "   - Sum these values across all clusters to get the total within-cluster dispersion matrix \\( W_k \\).\n",
        "\n",
        "3. **Calculate the Between-Cluster Dispersion**:\n",
        "   - For each cluster, calculate the squared distance between the cluster's centroid and the global centroid.\n",
        "   - Multiply this squared distance by the number of points in the cluster to obtain the total between-cluster dispersion matrix \\( B_k \\).\n",
        "\n",
        "4. **Compute the CH Score**:\n",
        "   - Use the formula mentioned above to compute the CH score by calculating the trace of the dispersion matrices and plugging them into the formula.\n",
        "\n"
      ],
      "metadata": {
        "id": "Jl2EPtrn1-yN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "def compute_ch_score(X, labels):\n",
        "    \"\"\"\n",
        "    Compute the Calinski-Harabasz (CH) score manually.\n",
        "\n",
        "    Parameters:\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            The data points.\n",
        "        labels : ndarray, shape (n_samples,)\n",
        "            The cluster labels for each data point.\n",
        "\n",
        "    Returns:\n",
        "        ch_score : float\n",
        "            The Calinski-Harabasz (CH) score.\n",
        "    \"\"\"\n",
        "    # Number of samples and number of clusters\n",
        "    n_samples, n_features = X.shape\n",
        "    k = len(np.unique(labels))\n",
        "\n",
        "    # Compute the overall centroid (mean of all data points)\n",
        "    global_centroid = np.mean(X, axis=0)\n",
        "\n",
        "    # Initialize matrices to calculate within and between cluster dispersion\n",
        "    W_k = np.zeros((n_features, n_features))  # Within-cluster dispersion matrix\n",
        "    B_k = np.zeros((n_features, n_features))  # Between-cluster dispersion matrix\n",
        "\n",
        "    # Iterate through each cluster\n",
        "    for i in range(k):\n",
        "        # Select points in the current cluster\n",
        "        cluster_points = X[labels == i]\n",
        "        cluster_size = len(cluster_points)\n",
        "\n",
        "        # Compute the centroid of the current cluster\n",
        "        cluster_centroid = np.mean(cluster_points, axis=0)\n",
        "\n",
        "        # Within-cluster dispersion: sum of squared distances from points to cluster centroid\n",
        "        W_k += np.dot((cluster_points - cluster_centroid).T, (cluster_points - cluster_centroid))\n",
        "\n",
        "        # Between-cluster dispersion: sum of squared distances from cluster centroid to global centroid\n",
        "        B_k += cluster_size * np.dot((cluster_centroid - global_centroid).T, (cluster_centroid - global_centroid))\n",
        "\n",
        "    # Compute the trace of both dispersion matrices\n",
        "    tr_W_k = np.trace(W_k)\n",
        "    tr_B_k = np.trace(B_k)\n",
        "\n",
        "    # Compute the Calinski-Harabasz Score\n",
        "    ch_score = (tr_B_k / (k - 1)) / (tr_W_k / (n_samples - k))\n",
        "\n",
        "    return ch_score\n",
        "\n",
        "# Create a synthetic dataset with clusters\n",
        "X, y = make_blobs(n_samples=100, centers=3, random_state=42)\n",
        "\n",
        "# Perform KMeans clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Compute the Calinski-Harabasz Score manually\n",
        "ch_score_manual = compute_ch_score(X, kmeans.labels_)\n",
        "\n",
        "print(f\"Manual Calinski-Harabasz Score: {ch_score_manual}\")\n"
      ],
      "metadata": {
        "id": "bUJWTgta5XEF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}